{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fbc18-faf3-4fc1-84e3-e171bfbe6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#　dev限定\n",
    "import sys\n",
    "sys.argv = [sys.argv[0]] \n",
    "sys.argv += ['--env', 'dev'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783295a6-e15f-4f53-b0e2-dff6320ca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField, BooleanType\n",
    "import pyspark\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "import pyspark.pandas as ps\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# pandas udf サンプル\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"age_sai\", StringType(), True)\n",
    "])\n",
    "\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    ages = pdf.age\n",
    "    return pdf.assign(age_sai=ages+\"歳\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8612a57-e1df-4e2f-9255-c1db91cdc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf設定\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['env'])\n",
    "\n",
    "# 環境分離 例\n",
    "if args['env'] == 'dev':\n",
    "    prefix='file://'\n",
    "else:\n",
    "    prefis='s3://'\n",
    "\n",
    "# ファイルの読み込み例\n",
    "spark_df = spark.read.csv(prefix + '/home/glue_user/workspace/jupyter_workspace/test.csv', header=True)\n",
    "spark_df=spark_df.withColumn('file_name', input_file_name())\n",
    "\n",
    "#guroup（id）ごとに何かの処理を適用\n",
    "#pandas udfを読んでいる\n",
    "kekka_spark_df = spark_df.groupby(\"id\").applyInPandas(subtract_mean, schema=schema)\n",
    "\n",
    "\n",
    "# parquet書き込み\n",
    "kekka_spark_df.write.mode('overwrite').parquet(prefix + '/home/glue_user/workspace/jupyter_workspace/data/2023-11-11/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27db66-7e71-4ca7-9951-28bae906655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(prefix + '/home/glue_user/workspace/jupyter_workspace/data/2023-11-11/output/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453ce31-195a-40d7-8f15-edbd592e1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3種類のデータフレームが登場する\n",
    "## pandas dataframe(pandas_df)\n",
    "## spark dataframe(spark_df)\n",
    "## pandas dataframe on spark(pandas_df_on_spark)\n",
    "## 分散されるのは後者２つ\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sparkのdataframeで読み込む\n",
    "spark_df=spark.read.csv(prefix + '/home/glue_user/workspace/jupyter_workspace/test.csv', header=True)\n",
    "\n",
    "# pandasのデータフレーム(普通のpandasnなので悪手)\n",
    "#pandas_df = spark_df.select(\"*\").toPandas()\n",
    "# これはダメ（普通のpandas操作になってる）\n",
    "#pandas_on_spark_df.asfreq()\n",
    "\n",
    "# pandas dataframe on spark\n",
    "import pyspark.pandas as ps\n",
    "pandas_df_on_spark = ps.DataFrame({'id': range(10)}, index=range(10))\n",
    "# pandasだけどsparknizeされる\n",
    "pandas_df_on_spark = pandas_df_on_spark.to_spark(index_col='index')\n",
    "\n",
    "#これはOK\n",
    "#ちなみにfilterはspark native\n",
    "pandas_df_on_spark = pandas_df_on_spark.filter(\"id > 5\")\n",
    "pandas_df_on_spark\n",
    "pandas_df_on_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09794166-68be-445a-bd1f-dd06f5ded99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_on_spark = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pdf):\n",
    "    return pdf + 1\n",
    "\n",
    "# transfrom型なのでSeries単位がinputになってoutputもSeries\n",
    "pandas_df_on_spark.pandas_on_spark.transform_batch(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a41c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def length(pdf) -> ps.DataFrame[int, [int]]:\n",
    "    return pd.DataFrame([len(pdf)])\n",
    "\n",
    "pandas_df_on_spark = ps.DataFrame({'A': range(1000)})\n",
    "\n",
    "# apply型なのでDataframe単位がinputになってoutputもDataframe\n",
    "pandas_df_on_spark.pandas_on_spark.apply_batch(length)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
