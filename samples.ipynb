{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fbc18-faf3-4fc1-84e3-e171bfbe6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#　dev限定\n",
    "import sys\n",
    "sys.argv = [sys.argv[0]] \n",
    "sys.argv += ['--env', 'dev'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783295a6-e15f-4f53-b0e2-dff6320ca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField, BooleanType\n",
    "import pyspark\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "import pyspark.pandas as ps\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# pandas udf サンプル\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"age_sai\", StringType(), True)\n",
    "])\n",
    "\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    ages = pdf.age\n",
    "    return pdf.assign(age_sai=ages+\"歳\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8612a57-e1df-4e2f-9255-c1db91cdc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf設定\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['env'])\n",
    "\n",
    "# 環境分離 例\n",
    "if args['env'] == 'dev':\n",
    "    prefix='file://'\n",
    "else:\n",
    "    prefis='s3://'\n",
    "\n",
    "# ファイルの読み込み例\n",
    "spark_df = spark.read.csv(prefix + '/home/glue_user/workspace/jupyter_workspace/test.csv', header=True)\n",
    "spark_df=spark_df.withColumn('file_name', input_file_name())\n",
    "\n",
    "#guroup（id）ごとに何かの処理を適用\n",
    "#pandas udfを呼んでいる\n",
    "#pandas udf例\n",
    "kekka_spark_df = spark_df.groupby(\"id\").applyInPandas(subtract_mean, schema=schema)\n",
    "\n",
    "\n",
    "# parquet書き込み（with snappy）\n",
    "kekka_spark_df.write.option(\"compression\", \"snappy\").mode('overwrite').parquet(prefix + '/home/glue_user/workspace/jupyter_workspace/data/2023-11-11/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27db66-7e71-4ca7-9951-28bae906655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(prefix + '/home/glue_user/workspace/jupyter_workspace/data/2023-11-11/output/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc27263-8102-473e-88f4-c2f340887249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkのdataframeで読み込む\n",
    "spark_df=spark.read.csv(prefix + '/home/glue_user/workspace/jupyter_workspace/test2.csv.bz2', header=True)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453ce31-195a-40d7-8f15-edbd592e1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3種類のデータフレームが登場する\n",
    "## pandas dataframe(pandas_df)\n",
    "## spark dataframe(spark_df)\n",
    "## pandas dataframe on spark(pandas_df_on_spark)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pandasのデータフレーム(普通のpandasnなので悪手)\n",
    "#pandas_df = spark_df.select(\"*\").toPandas()\n",
    "# これはダメ（普通のpandas操作になってる）\n",
    "#pandas_on_spark_df.asfreq()\n",
    "\n",
    "# pandas dataframe on spark\n",
    "# pandasはpandasでもこれはOK\n",
    "import pyspark.pandas as ps\n",
    "pandas_df_on_spark = ps.DataFrame({'id': range(10)}, index=range(10))\n",
    "# pandasだけどsparknizeされる\n",
    "pandas_df_on_spark = pandas_df_on_spark.to_spark(index_col='index')\n",
    "\n",
    "#これはOK\n",
    "#ちなみにfilterはspark native\n",
    "pandas_df_on_spark = pandas_df_on_spark.filter(\"id > 5\")\n",
    "pandas_df_on_spark\n",
    "pandas_df_on_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a33dfd-ba97-4fc9-89c0-c693f502d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import logging\n",
    "# geopyは存在しない\n",
    "# from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09794166-68be-445a-bd1f-dd06f5ded99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_on_spark = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n",
    "def pandas_plus(pdf):\n",
    "    return pdf + 1\n",
    "\n",
    "# transfrom型なのでSeries単位がinputになってoutputもSeries\n",
    "pandas_df_on_spark.pandas_on_spark.transform_batch(pandas_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe8ed2-237c-408e-96e7-9c856b49149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a41c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def length(pdf) -> ps.DataFrame[int, [int]]:\n",
    "    return pd.DataFrame([len(pdf)])\n",
    "\n",
    "pandas_df_on_spark = ps.DataFrame({'A': range(1000)})\n",
    "\n",
    "# apply型なのでDataframe単位がinputになってoutputもDataframe\n",
    "pandas_df_on_spark.pandas_on_spark.apply_batch(length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842ad85-803b-4730-bd48-b385e22a0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas -> spark\n",
    "pdf = spark_df.toPandas() \n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75150ba-e14d-4d66-b805-3c775f219305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_dfをpandas_on_sparkkへ変換する\n",
    "psdf = spark_df.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54843a16-86b6-4c21-b725-6e7fa7d64136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#普通にpandasぽく使える（サポートされているなら）\n",
    "# サポートされているものはpandas_udfに優先して使う\n",
    "psdf.iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58163ca8-69cf-4452-a166-a62cef19ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#次に置き換えを考える\n",
    "#rolling -> windowで対応できる\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "avg_val = max(col(\"age\")).over(Window.partitionBy(col(\"id\")))\n",
    "spark_df.withColumn(\"max_age\",avg_val).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2f4af-1848-4109-9bdb-2de77c2ccbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サポートしていないものはpandas udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Sparkセッションの作成\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# サンプルデータの作成\n",
    "data = [(1, '2023-01-01', 10.0),\n",
    "        (2, '2023-01-02', 20.0),\n",
    "        (3, '2023-01-03', 30.0)]\n",
    "schema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                     StructField(\"date\", StringType(), True),\n",
    "                     StructField(\"value\", DoubleType(), True)])\n",
    "schema2 = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                     StructField(\"date\", TimestampType(), True),\n",
    "                     StructField(\"value\", DoubleType(), True)])\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# 日付列をTimestamp型に変換\n",
    "df = df.withColumn(\"date\", df[\"date\"].cast(\"timestamp\"))\n",
    "\n",
    "# Pandas UDFでasfreqメソッドを実行\n",
    "def asfreq_pandas_udf(pdf):\n",
    "    pdf['date'] = pd.to_datetime(pdf['date'])\n",
    "    pdf = pdf.set_index('date').asfreq('W')  # ここでasfreqメソッドを呼び出す\n",
    "    return pdf.reset_index()\n",
    "\n",
    "result_df = df.groupby(\"id\").applyInPandas(asfreq_pandas_udf, schema=schema2)\n",
    "\n",
    "# 結果を表示\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
